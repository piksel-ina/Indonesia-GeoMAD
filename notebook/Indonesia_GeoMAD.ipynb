{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Indonesia GeoMAD Notebook\n",
    "subtitle: Testing ODC-Stats Configuration for Cloud Cover Optimization and Picking the Right Study and Testing Area\n",
    "date: 2025-11-13\n",
    "authors:\n",
    "  - name: Muhammad Taufik\n",
    "    affiliations:\n",
    "      - Badan Informasi Geospasial (BIG)\n",
    "    email: muhammad.taufik@big.go.id\n",
    "  - name: Fang Yuan\n",
    "    affiliations:\n",
    "      - Auspatious\n",
    "    email: contact@fangyuan.space\n",
    "  - name: Alex Leith\n",
    "    affiliations:\n",
    "      - Auspatious\n",
    "    email: alex@auspatious.com\n",
    "\n",
    "keywords:\n",
    "  - GeoMAD\n",
    "  - Sentinel-2\n",
    "  - Open Data Cube\n",
    "  - Cloud Cover\n",
    "  - Indonesia\n",
    "  - Remote Sensing\n",
    "  - Earth Observation\n",
    "project:\n",
    "  license: CC-BY-4.0\n",
    "  open_access: true\n",
    "  github: https://github.com/piksel-ina/Indonesia-geomad\n",
    "kernelspec:\n",
    "  name: python3\n",
    "  display_name: Python 3\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This notebook explores optimal cloud cover thresholds for generating geoMAD (Geometric Median and Median Absolute Deviation) composites over Indonesia using Sentinel-2 L2A data. We compare different cloud cover filtering strategies (≤100%, ≤80%, ≤60%) to balance data quality and temporal coverage. Additionally, we evaluate suitable study areas for testing and validation across various Indonesia's geographic conditions.\n",
    "\n",
    "## A. Objectives\n",
    "\n",
    "1. Evaluate data distribution and availability across Indonesia under different cloud cover thresholds (100%, 80%, 60%)\n",
    "\n",
    "2. Locate tiles with least datasets to serve as test subjects alongside tiles with diverse geographic conditions\n",
    "\n",
    "3. Test with Argo Workflows to document peak memory usage, especially on high-dataset tiles\n",
    "\n",
    "\n",
    "## B. Initial Setup\n",
    "### Libraries Used\n",
    "pandas\n",
    ": Python data analysis library for handling tabular data, DataFrames, and statistical operations. We'll use this to analyze dataset distribution statistics.\n",
    "\n",
    "odc-stats\n",
    ": Open Data Cube statistics toolkit for generating temporal composites and summary statistics from Earth observation data. We'll execute this via terminal commands through Jupyter cells.\n",
    "\n",
    "matplotlib.pyplot\n",
    ": Python plotting library for creating visualizations. We'll use this to visualize sampling distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In piksel-sandbox, we need to upgrade odc-stats to the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade odc-stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Generate ODC-Stats Task Database\n",
    "\n",
    "> We use terminal commands to imitate the production workflow with odc-stats container.\n",
    "\n",
    "The function below generates task databases filtered by cloud cover threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tasks(cloud_cover, output_db):\n",
    "    !odc-stats save-tasks \\\n",
    "        --frequency \"annual\" \\\n",
    "        --grid \"EPSG:6933;10;5000\" \\\n",
    "        --year \"2024\" \\\n",
    "        --input-products \"s2_l2a\" \\\n",
    "        --dataset-filter='{{\"cloud_cover\": [0,{cloud_cover}]}}' \\\n",
    "        {output_db}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How save_tasks() Works\n",
    "\n",
    "When executed, the `save-tasks` command performs the following operations:\n",
    "\n",
    "1. **Query ODC Database** - Connects to the Open Data Cube and queries all indexed Sentinel-2 L2A datasets for the year 2024\n",
    "\n",
    "2. **Apply Cloud Cover Filter** - The function receives two arguments: `cloud_cover_threshold` and `output_filename`. It filters datasets based on the specified threshold:\n",
    "\n",
    "   ```python\n",
    "   save_tasks(60, \"tasks_cc60.db\")   # cloud_cover: [0, 60]\n",
    "   save_tasks(80, \"tasks_cc80.db\")   # cloud_cover: [0, 80]\n",
    "   save_tasks(100, \"tasks_cc100.db\") # cloud_cover: [0, 100]\n",
    "   ```\n",
    "   \n",
    "   - **First argument**: Maximum cloud cover percentage (60, 80, or 100)\n",
    "   - **Second argument**: Output filename prefix for generated files\n",
    "   - Filters include all datasets with cloud cover from 0% up to the specified threshold\n",
    "\n",
    "3. **Generate Spatial Grid** - Creates a processing grid in EPSG:6933 projection with 10° tiles at 5000m resolution covering Indonesia\n",
    "\n",
    "4. **Spatial Intersection** - Matches filtered datasets to their corresponding grid tiles based on spatial footprints\n",
    "\n",
    "5. **Task Generation** - For each tile, generates processing tasks containing:\n",
    "   - Tile identifier and spatial bounds\n",
    "   - List of datasets intersecting that tile\n",
    "   - Metadata for GeoMAD computation\n",
    "\n",
    "6. **Database Storage** - Serializes all tasks into multiple output formats:\n",
    "   - **`.db`** - SQLite database for efficient querying and task distribution\n",
    "   - **`.csv`** - Tabular summary of tiles and dataset counts\n",
    "   - **`.json`** - JSON manifest with complete task specifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function (This process takes several minutes to complete)\n",
    "save_tasks(60, \"tasks_cc60.db\")\n",
    "save_tasks(80, \"tasks_cc80.db\")\n",
    "save_tasks(100, \"tasks_cc100.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generated Task Files\n",
    "After executing the save_tasks() function with three different cloud cover thresholds, \n",
    "you should have **9 output files** in total, each cloud cover threshold produces **3 files**:\n",
    "\n",
    "| Cloud Cover | Database | CSV Summary | JSON Manifest |\n",
    "|-------------|----------|-------------|---------------|\n",
    "| 0-60% | `tasks_cc60.db` | `tasks_cc60.csv` | `tasks_cc60.json` |\n",
    "| 0-80% | `tasks_cc80.db` | `tasks_cc80.csv` | `tasks_cc80.json` |\n",
    "| 0-100% | `tasks_cc100.db` | `tasks_cc100.csv` | `tasks_cc100.json` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Working with the CSV Files\n",
    "Now that we have generated the task files, let's analyze the data distribution and availability across Indonesia under different cloud cover thresholds. This analysis will help us understand how cloud filtering impacts dataset availability and identify optimal processing strategies.\n",
    "\n",
    "### 1. Loading and Inspecting CSV Files\n",
    "Let's examine the contents of the CSV files using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file for 100% (complete dataset)\n",
    "df_cc100 = pd.read_csv(\"tasks_cc100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": [
     {
      "expression": "df_cc100.head()",
      "result": {
       "data": {
        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>T</th>\n      <th>X</th>\n      <th>Y</th>\n      <th>datasets</th>\n      <th>days</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2024--P1Y</td>\n      <td>182</td>\n      <td>-1</td>\n      <td>72</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2024--P1Y</td>\n      <td>182</td>\n      <td>0</td>\n      <td>72</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2024--P1Y</td>\n      <td>182</td>\n      <td>1</td>\n      <td>72</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2024--P1Y</td>\n      <td>182</td>\n      <td>2</td>\n      <td>145</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2024--P1Y</td>\n      <td>182</td>\n      <td>3</td>\n      <td>73</td>\n      <td>72</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
        "text/plain": "           T    X  Y  datasets  days\n0  2024--P1Y  182 -1        72    72\n1  2024--P1Y  182  0        72    72\n2  2024--P1Y  182  1        72    72\n3  2024--P1Y  182  2       145    72\n4  2024--P1Y  182  3        73    72"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "> The `pd.read_csv()` command is reading the CSV file and loading it into memory as a pandas DataFrame.\n",
    "\n",
    "`df_cc100` is a pandas DataFrame object with following rows and columns:\n",
    "\n",
    "{eval}`df_cc100.head()`\n",
    "\n",
    "Based on the dataframe output, the CSV files contain the following columns:\n",
    "- **T** - Time period identifier (e.g., \"2024--P1Y\" represents year 2024 with 1-year period)\n",
    "- **X** - Grid tile X-coordinate in the EPSG:6933 projection system\n",
    "- **Y** - Grid tile Y-coordinate in the EPSG:6933 projection system\n",
    "- **datasets** - Number of Sentinel-2 datasets intersecting this tile\n",
    "- **days** - Number of unique observation days available for this tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": [
     {
      "expression": "df_cc100.describe()",
      "result": {
       "data": {
        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>Y</th>\n      <th>datasets</th>\n      <th>days</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3755.000000</td>\n      <td>3755.000000</td>\n      <td>3755.000000</td>\n      <td>3755.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>228.991212</td>\n      <td>-6.718775</td>\n      <td>277.258589</td>\n      <td>124.297736</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>24.446351</td>\n      <td>13.297015</td>\n      <td>146.041108</td>\n      <td>33.815803</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>182.000000</td>\n      <td>-31.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>208.000000</td>\n      <td>-18.000000</td>\n      <td>152.000000</td>\n      <td>74.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>230.000000</td>\n      <td>-7.000000</td>\n      <td>229.000000</td>\n      <td>145.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>249.000000</td>\n      <td>4.000000</td>\n      <td>383.000000</td>\n      <td>146.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>274.000000</td>\n      <td>16.000000</td>\n      <td>764.000000</td>\n      <td>219.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
        "text/plain": "                 X            Y     datasets         days\ncount  3755.000000  3755.000000  3755.000000  3755.000000\nmean    228.991212    -6.718775   277.258589   124.297736\nstd      24.446351    13.297015   146.041108    33.815803\nmin     182.000000   -31.000000     1.000000     1.000000\n25%     208.000000   -18.000000   152.000000    74.000000\n50%     230.000000    -7.000000   229.000000   145.000000\n75%     249.000000     4.000000   383.000000   146.000000\nmax     274.000000    16.000000   764.000000   219.000000"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{len(df_cc100):,}\"",
      "result": {
       "data": {
        "text/plain": "'3,755'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['datasets'].mean():.2f}\"",
      "result": {
       "data": {
        "text/plain": "'277.26'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['datasets'].median():.0f}\"",
      "result": {
       "data": {
        "text/plain": "'229'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['datasets'].min():.0f}\"",
      "result": {
       "data": {
        "text/plain": "'1'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['datasets'].max():.0f}\"",
      "result": {
       "data": {
        "text/plain": "'764'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['datasets'].std():.2f}\"",
      "result": {
       "data": {
        "text/plain": "'146.04'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['days'].mean():.2f}\"",
      "result": {
       "data": {
        "text/plain": "'124.30'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['days'].median():.0f}\"",
      "result": {
       "data": {
        "text/plain": "'145'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['days'].min():.0f}\"",
      "result": {
       "data": {
        "text/plain": "'1'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['days'].max():.0f}\"",
      "result": {
       "data": {
        "text/plain": "'219'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['days'].std():.2f}\"",
      "result": {
       "data": {
        "text/plain": "'33.82'"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "### 2. The Descriptive Statistics\n",
    "\n",
    "Pandas gives us a statistical snapshot of our data; central tendency (mean, median), spread (std, quartiles), and range (min, max). Perfect for understanding the dataset distribution at a glance.\n",
    "\n",
    "```python\n",
    "df_cc100.describe()\n",
    "```\n",
    "> Here we'll try to extract general information about the dataset `df_cc100`.\n",
    "\n",
    "**Dataframe Statistic Summary:** \\\n",
    "{eval}`df_cc100.describe()`\n",
    "\n",
    ":::{important} Total tiles is **{eval}`f\"{len(df_cc100):,}\"`**\n",
    "\n",
    "We can also observe some more information about the dataset:  \n",
    "\n",
    "1. **Datasets per Tile**\n",
    "\n",
    "    - _Mean_: {eval}`f\"{df_cc100['datasets'].mean():.2f}\"` datasets per tile\n",
    "    - _Median_: {eval}`f\"{df_cc100['datasets'].median():.0f}\"` datasets per tile\n",
    "    - _Range_: {eval}`f\"{df_cc100['datasets'].min():.0f}\"` to {eval}`f\"{df_cc100['datasets'].max():.0f}\"` datasets\n",
    "    - _Standard deviation_: {eval}`f\"{df_cc100['datasets'].std():.2f}\"`\n",
    "\n",
    "2. **Observation Days per Tile**\n",
    "\n",
    "    - _Mean_: {eval}`f\"{df_cc100['days'].mean():.2f}\"` days per tile\n",
    "    - _Median_: {eval}`f\"{df_cc100['days'].median():.0f}\"` days per tile\n",
    "    - _Range_: {eval}`f\"{df_cc100['days'].min():.0f}\"` to {eval}`f\"{df_cc100['days'].max():.0f}\"` days\n",
    "    - _Standard deviation_: {eval}`f\"{df_cc100['days'].std():.2f}\"`\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Create horizontal boxplot\n",
    "bp = ax.boxplot([df_cc100['datasets'], df_cc100['days']], \n",
    "                 vert=False,\n",
    "                 tick_labels=['Datasets', 'Days'],\n",
    "                 patch_artist=True)\n",
    "\n",
    "# Set colors\n",
    "colors = ['#3498db', '#f39c12']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_xlabel('Count', fontsize=12)\n",
    "ax.set_ylabel('Metric', fontsize=12)\n",
    "ax.set_title('Distribution of Datasets and Observation Days per Tile', \n",
    "             fontsize=13, fontweight='bold', pad=15)\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{grid} 2\n",
    ":::{card}\n",
    ":header: **What the Data Shows**\n",
    "\n",
    "Most tiles contain 150-380 datasets spanning 74-146 observation days, with a median of 229 datasets across 145 days. Dataset counts vary significantly more than observation duration, with some tiles receiving multiple satellite passes per day (average 1.6 datasets/day). The spatial distribution is uneven—some areas have substantially more data than others due to satellite orbit patterns.\n",
    ":::\n",
    "\n",
    ":::{card}\n",
    ":header: **Implications for GeoMAD Analysis**\n",
    "\n",
    "Data-rich tiles will produce more robust median and MAD statistics, while tiles with fewer observations (around 74 days) may not capture full seasonal variations for annual analysis. The spatial imbalance means geoMAD composite quality varies across the region. Areas with limited temporal coverage may be more susceptible to outliers or cloud contamination, affecting the reliability of statistical summaries when comparing results between tiles.\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Cloud Cover Filter Analysis\n",
    "\n",
    "We apply the same analytical method to filtered datasets (`tasks_cc60.csv` and `tasks_cc80.csv`) to evaluate the impact of cloud cover thresholds on data availability and quality.\n",
    "\n",
    "\n",
    "- **CC60** applies a more aggressive cloud cover filter (≤60% cloud cover allowed)  \n",
    "- **CC80** applies a more lenient filter (≤80% cloud cover allowed)\n",
    "\n",
    "\n",
    "### 1. The Question\n",
    "\n",
    "**How do we balance output quality with dataset availability?**\n",
    "\n",
    "Larger cloud cover filters reduce the number of cloudy datasets, which should improve output image quality. However, we must maintain a minimum dataset count to provide sufficient information for robust statistical analysis, particularly for **Median Absolute Deviation (MAD)** calculations.\n",
    "\n",
    "\n",
    "### 2. Comparative Analysis\n",
    "\n",
    "We compare two configurations (60% and 80% cloud cover thresholds) across two dimensions:\n",
    "\n",
    "::::{grid} 2\n",
    ":gutter: 3\n",
    "\n",
    ":::{grid-item-card} Dataset Count Analysis\n",
    ":class-header: bg-light\n",
    "\n",
    "- Number of available datasets per tile\n",
    "- Temporal coverage consistency\n",
    "- Spatial distribution patterns\n",
    "- Data retention rates vs. baseline (CC100)\n",
    "\n",
    ":::\n",
    "\n",
    ":::{grid-item-card} Output Quality Assessment\n",
    ":class-header: bg-light\n",
    "\n",
    "- Image clarity and cloud contamination\n",
    "- Statistical robustness (MAD reliability)\n",
    "- Spatial completeness\n",
    "- Temporal representativeness\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "### 3. Expected Trade-offs\n",
    "\n",
    "```{list-table}\n",
    ":header-rows: 1\n",
    ":name: tradeoff-table\n",
    "\n",
    "* - Metric\n",
    "  - CC60 (Aggressive)\n",
    "  - CC80 (Lenient)\n",
    "* - Dataset Count\n",
    "  - Lower\n",
    "  - Higher\n",
    "* - Cloud Contamination\n",
    "  - Minimal\n",
    "  - Moderate\n",
    "* - Statistical Power\n",
    "  - Risk of insufficient data\n",
    "  - Better temporal sampling\n",
    "* - Output Quality\n",
    "  - Cleaner imagery\n",
    "  - Potential artifacts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the other CSV files \n",
    "df_cc80 = pd.read_csv(\"tasks_cc80.csv\")\n",
    "df_cc60 = pd.read_csv(\"tasks_cc60.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": [
     {
      "expression": "df_cc80.describe()",
      "result": {
       "data": {
        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>Y</th>\n      <th>datasets</th>\n      <th>days</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3749.000000</td>\n      <td>3749.000000</td>\n      <td>3749.000000</td>\n      <td>3749.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>229.014137</td>\n      <td>-6.689250</td>\n      <td>162.220592</td>\n      <td>81.488930</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>24.455164</td>\n      <td>13.286465</td>\n      <td>92.374814</td>\n      <td>26.888991</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>182.000000</td>\n      <td>-31.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>208.000000</td>\n      <td>-18.000000</td>\n      <td>92.000000</td>\n      <td>58.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>230.000000</td>\n      <td>-7.000000</td>\n      <td>140.000000</td>\n      <td>86.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>249.000000</td>\n      <td>4.000000</td>\n      <td>223.000000</td>\n      <td>102.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>274.000000</td>\n      <td>16.000000</td>\n      <td>550.000000</td>\n      <td>179.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
        "text/plain": "                 X            Y     datasets         days\ncount  3749.000000  3749.000000  3749.000000  3749.000000\nmean    229.014137    -6.689250   162.220592    81.488930\nstd      24.455164    13.286465    92.374814    26.888991\nmin     182.000000   -31.000000     1.000000     1.000000\n25%     208.000000   -18.000000    92.000000    58.000000\n50%     230.000000    -7.000000   140.000000    86.000000\n75%     249.000000     4.000000   223.000000   102.000000\nmax     274.000000    16.000000   550.000000   179.000000"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "df_cc60.describe()",
      "result": {
       "data": {
        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>Y</th>\n      <th>datasets</th>\n      <th>days</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3749.000000</td>\n      <td>3749.000000</td>\n      <td>3749.000000</td>\n      <td>3749.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>229.014137</td>\n      <td>-6.689250</td>\n      <td>121.142971</td>\n      <td>64.016538</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>24.455164</td>\n      <td>13.286465</td>\n      <td>76.255768</td>\n      <td>25.470309</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>182.000000</td>\n      <td>-31.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>208.000000</td>\n      <td>-18.000000</td>\n      <td>65.000000</td>\n      <td>44.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>230.000000</td>\n      <td>-7.000000</td>\n      <td>103.000000</td>\n      <td>64.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>249.000000</td>\n      <td>4.000000</td>\n      <td>161.000000</td>\n      <td>82.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>274.000000</td>\n      <td>16.000000</td>\n      <td>506.000000</td>\n      <td>172.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
        "text/plain": "                 X            Y     datasets         days\ncount  3749.000000  3749.000000  3749.000000  3749.000000\nmean    229.014137    -6.689250   121.142971    64.016538\nstd      24.455164    13.286465    76.255768    25.470309\nmin     182.000000   -31.000000     1.000000     1.000000\n25%     208.000000   -18.000000    65.000000    44.000000\n50%     230.000000    -7.000000   103.000000    64.000000\n75%     249.000000     4.000000   161.000000    82.000000\nmax     274.000000    16.000000   506.000000   172.000000"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "### 4. Data Availibility and Distribution\n",
    ":::: {grid} 2\n",
    "::: {grid-item}\n",
    "```{code}python\n",
    ":label: descriptive-analysis-cloud-cover-80\n",
    ":caption: descriptive statistics of filtered dataset with cloud-cover 0 to 80%\n",
    "df_cc80.describe()\n",
    "```\n",
    "\n",
    "\n",
    "{eval}`df_cc80.describe()`\n",
    ":::\n",
    "\n",
    "::: {grid-item}\n",
    "```{code}python\n",
    ":label: descriptive-analysis-cloud-cover-60\n",
    ":caption: descriptive statistics of filtered dataset with cloud-cover 0 to 60%\n",
    "df_cc60.describe()\n",
    "```\n",
    "\n",
    "\n",
    "{eval}`df_cc60.describe()`\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw={'wspace': 0.5})\n",
    "\n",
    "colors = ['#3498db', '#f39c12']\n",
    "\n",
    "# Column 1: Datasets comparison\n",
    "bp1 = ax1.boxplot([df_cc80['datasets'], df_cc60['datasets']], \n",
    "                   vert=False,\n",
    "                   tick_labels=['80CC', '60CC'],\n",
    "                   patch_artist=True)\n",
    "\n",
    "for patch, color in zip(bp1['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax1.set_xlabel('Dataset Count per Tile', fontsize=12)\n",
    "ax1.set_ylabel('CC Threshold', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Column 2: Days comparison\n",
    "bp2 = ax2.boxplot([df_cc80['days'], df_cc60['days']], \n",
    "                   vert=False,\n",
    "                   tick_labels=['80CC', '60CC'],\n",
    "                   patch_artist=True)\n",
    "\n",
    "for patch, color in zip(bp2['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax2.set_xlabel('Observation Days Count per Tile', fontsize=12)\n",
    "ax2.set_ylabel('CC Threshold', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cloud Cover Threshold Analysis\n",
    "\n",
    ":::{card}\n",
    ":header: **Overview**\n",
    "- We analyzed three cloud cover thresholds (100%, 80%, 60%) to determine optimal filtering for our satellite imagery dataset.\n",
    "- The analysis reveals that **80% threshold offers the best balance**, though **60% remains acceptable**.\n",
    ":::\n",
    "\n",
    "::::{grid} 1 1 3 3\n",
    ":gutter: 3\n",
    "\n",
    ":::{card} Data Coverage Impact\n",
    "**Location Retention:**\n",
    "- **100%**: 3,755 locations\n",
    "- **80%**: 3,749 locations (99.8%)\n",
    "- **60%**: 3,749 locations (99.8%)\n",
    "\n",
    "**Key Insight:** Loss of only 6 locations indicates nearly all grid points have sufficient usable data, even with stricter cloud filtering.\n",
    ":::\n",
    "\n",
    ":::{card} Dataset Availability\n",
    "**Mean / Median Datasets:**\n",
    "- **100%**: 277 / 229\n",
    "- **80%**: 162 / 140 (-41.5%)\n",
    "- **60%**: 121 / 103 (-56.3%)\n",
    "\n",
    "**Key Insight:** 80% removes ~42% of datasets while maintaining 140 median images per location. 60% removes over half the data but still provides 103 median images - still sufficient for analysis.\n",
    ":::\n",
    "\n",
    ":::{card} Temporal Coverage\n",
    "**Mean / Median Days:**\n",
    "- **100%**: 124 / 145 days\n",
    "- **80%**: 81 / 86 days (-34.7%)\n",
    "- **60%**: 64 / 64 days (-48.5%)\n",
    "\n",
    "**Key Insight:** 80% provides 86 median days - sufficient for seasonal analysis. 60% reduces to 64 days (~2 months coverage) which may create temporal gaps but ensures each observation is high-quality.\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    ":::{card} \n",
    ":header: **Next Steps**\n",
    "To make a final decision, we need to **visually compare outputs** from different thresholds by sampling representative tiles based on dataset and day count distributions.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Testing Strategy\n",
    "\n",
    "### 1. Sampling Strategy\n",
    "\n",
    "To finalize the threshold decision, we need to **visually compare outputs** by sampling representative tiles.\n",
    "\n",
    "::::{grid} 1 1 2 2\n",
    ":gutter: 3\n",
    "\n",
    ":::{card} \n",
    ":header: **Sampling Dimensions**\n",
    "1. Dataset Count Bins:\n",
    "    - Low: < 25th percentile\n",
    "    - Medium: 25th-75th percentile  \n",
    "    - High: > 75th percentile\n",
    "\n",
    "2. Day Count Bins:\n",
    "    - Low: < 25th percentile\n",
    "    - Medium: 25th-75th percentile\n",
    "    - High: > 75th percentile\n",
    ":::\n",
    "\n",
    ":::{card} \n",
    ":header: **Test Distribution**\n",
    "\n",
    "Create a **3×3 sampling matrix**:\n",
    "\n",
    "| Days \\ Datasets | Low | Med | High |\n",
    "|----------------|-----|-----|------|\n",
    "| **Low** | 2 tiles | 2 tiles | 2 tiles |\n",
    "| **Medium** | 2 tiles | 3 tiles | 2 tiles |\n",
    "| **High** | 2 tiles | 2 tiles | 2 tiles |\n",
    "\n",
    "- Total Tiles: **19 tiles**\n",
    "- Total Processing:\n",
    "    - 19 tiles at 80% & 60% = 38 images\n",
    "    - 5 tiles additionally at 100% = 5 images\n",
    "    - Total output: **43 images**\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "::::{grid} 1 1 2 2\n",
    ":gutter: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
