{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Indonesia GeoMAD Notebook\n",
    "subtitle: Testing ODC-Stats Configuration for Cloud Cover Optimization and Picking the Right Study and Testing Area\n",
    "date: 2025-11-13\n",
    "authors:\n",
    "  - name: Muhammad Taufik\n",
    "    affiliations:\n",
    "      - Badan Informasi Geospasial (BIG)\n",
    "    email: muhammad.taufik@big.go.id\n",
    "  - name: Fang Yuan\n",
    "    affiliations:\n",
    "      - Auspatious\n",
    "    email: contact@fangyuan.space\n",
    "  - name: Alex Leith\n",
    "    affiliations:\n",
    "      - Auspatious\n",
    "    email: alex@auspatious.com\n",
    "\n",
    "keywords:\n",
    "  - GeoMAD\n",
    "  - Sentinel-2\n",
    "  - Open Data Cube\n",
    "  - Cloud Cover\n",
    "  - Indonesia\n",
    "  - Remote Sensing\n",
    "  - Earth Observation\n",
    "project:\n",
    "  license: CC-BY-4.0\n",
    "  open_access: true\n",
    "  github: https://github.com/piksel-ina/Indonesia-geomad\n",
    "kernelspec:\n",
    "  name: python3\n",
    "  display_name: Python 3\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This notebook explores optimal cloud cover thresholds for generating geoMAD (Geometric Median and Median Absolute Deviation) composites over Indonesia using Sentinel-2 L2A data. We compare different cloud cover filtering strategies (≤100%, ≤80%, ≤60%) to balance data quality and temporal coverage. Additionally, we evaluate suitable study areas for testing and validation across various Indonesia's geographic conditions.\n",
    "\n",
    "## A. Objectives\n",
    "\n",
    "1. Evaluate data distribution and availability across Indonesia under different cloud cover thresholds (100%, 80%, 60%)\n",
    "\n",
    "2. Locate tiles with least datasets to serve as test subjects alongside tiles with diverse geographic conditions\n",
    "\n",
    "3. Test with Argo Workflows to document peak memory usage, especially on high-dataset tiles\n",
    "\n",
    "\n",
    "## B. Initial Setup\n",
    "### Libraries Used\n",
    "pandas\n",
    ": Python data analysis library for handling tabular data, DataFrames, and statistical operations. We'll use this to analyze dataset distribution statistics.\n",
    "\n",
    "odc-stats\n",
    ": Open Data Cube statistics toolkit for generating temporal composites and summary statistics from Earth observation data. We'll execute this via terminal commands through Jupyter cells.\n",
    "\n",
    "matplotlib.pyplot\n",
    ": Python plotting library for creating visualizations. We'll use this to visualize sampling distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from matplotlib.patches import Rectangle, Patch\n",
    "import contextily as ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In piksel-sandbox, we need to upgrade odc-stats to the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade odc-stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Generate ODC-Stats Task Database\n",
    "\n",
    "> We use terminal commands to imitate the production workflow with odc-stats container.\n",
    "\n",
    "The function below generates task databases filtered by cloud cover threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tasks(cloud_cover, output_db):\n",
    "    !odc-stats save-tasks \\\n",
    "        --frequency \"annual\" \\\n",
    "        --grid \"EPSG:6933;10;5000\" \\\n",
    "        --year \"2024\" \\\n",
    "        --input-products \"s2_l2a\" \\\n",
    "        --dataset-filter='{{\"cloud_cover\": [0,{cloud_cover}]}}' \\\n",
    "        {output_db}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How save_tasks() Works\n",
    "\n",
    "When executed, the `save-tasks` command performs the following operations:\n",
    "\n",
    "1. **Query ODC Database** - Connects to the Open Data Cube and queries all indexed Sentinel-2 L2A datasets for the year 2024\n",
    "\n",
    "2. **Apply Cloud Cover Filter** - The function receives two arguments: `cloud_cover_threshold` and `output_filename`. It filters datasets based on the specified threshold:\n",
    "\n",
    "   ```python\n",
    "   save_tasks(60, \"tasks_cc60.db\")   # cloud_cover: [0, 60]\n",
    "   save_tasks(80, \"tasks_cc80.db\")   # cloud_cover: [0, 80]\n",
    "   save_tasks(100, \"tasks_cc100.db\") # cloud_cover: [0, 100]\n",
    "   ```\n",
    "   \n",
    "   - **First argument**: Maximum cloud cover percentage (60, 80, or 100)\n",
    "   - **Second argument**: Output filename prefix for generated files\n",
    "   - Filters include all datasets with cloud cover from 0% up to the specified threshold\n",
    "\n",
    "3. **Generate Spatial Grid** - Creates a processing grid in EPSG:6933 projection with 10° tiles at 5000m resolution covering Indonesia\n",
    "\n",
    "4. **Spatial Intersection** - Matches filtered datasets to their corresponding grid tiles based on spatial footprints\n",
    "\n",
    "5. **Task Generation** - For each tile, generates processing tasks containing:\n",
    "   - Tile identifier and spatial bounds\n",
    "   - List of datasets intersecting that tile\n",
    "   - Metadata for GeoMAD computation\n",
    "\n",
    "6. **Database Storage** - Serializes all tasks into multiple output formats:\n",
    "   - **`.db`** - SQLite database for efficient querying and task distribution\n",
    "   - **`.csv`** - Tabular summary of tiles and dataset counts\n",
    "   - **`.json`** - JSON manifest with complete task specifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function (This process takes several minutes to complete)\n",
    "save_tasks(60, \"tasks_cc60.db\")\n",
    "save_tasks(80, \"tasks_cc80.db\")\n",
    "save_tasks(100, \"tasks_cc100.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generated Task Files\n",
    "After executing the save_tasks() function with three different cloud cover thresholds, \n",
    "you should have **9 output files** in total, each cloud cover threshold produces **3 files**:\n",
    "\n",
    "| Cloud Cover | Database | CSV Summary | JSON Manifest |\n",
    "|-------------|----------|-------------|---------------|\n",
    "| 0-60% | `tasks_cc60.db` | `tasks_cc60.csv` | `tasks_cc60.json` |\n",
    "| 0-80% | `tasks_cc80.db` | `tasks_cc80.csv` | `tasks_cc80.json` |\n",
    "| 0-100% | `tasks_cc100.db` | `tasks_cc100.csv` | `tasks_cc100.json` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Working with the CSV Files\n",
    "Now that we have generated the task files, let's analyze the data distribution and availability across Indonesia under different cloud cover thresholds. This analysis will help us understand how cloud filtering impacts dataset availability and identify optimal processing strategies.\n",
    "\n",
    "### 1. Loading and Inspecting CSV Files\n",
    "Let's examine the contents of the CSV files using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file for 100% (complete dataset)\n",
    "df_cc100 = pd.read_csv(\"tasks_cc100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": [
     {
      "expression": "df_cc100.head()",
      "result": {
       "data": {
        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>T</th>\n      <th>X</th>\n      <th>Y</th>\n      <th>datasets</th>\n      <th>days</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2024--P1Y</td>\n      <td>182</td>\n      <td>-1</td>\n      <td>72</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2024--P1Y</td>\n      <td>182</td>\n      <td>0</td>\n      <td>72</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2024--P1Y</td>\n      <td>182</td>\n      <td>1</td>\n      <td>72</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2024--P1Y</td>\n      <td>182</td>\n      <td>2</td>\n      <td>145</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2024--P1Y</td>\n      <td>182</td>\n      <td>3</td>\n      <td>73</td>\n      <td>72</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
        "text/plain": "           T    X  Y  datasets  days\n0  2024--P1Y  182 -1        72    72\n1  2024--P1Y  182  0        72    72\n2  2024--P1Y  182  1        72    72\n3  2024--P1Y  182  2       145    72\n4  2024--P1Y  182  3        73    72"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "> The `pd.read_csv()` command is reading the CSV file and loading it into memory as a pandas DataFrame.\n",
    "\n",
    "`df_cc100` is a pandas DataFrame object with following rows and columns:\n",
    "\n",
    "{eval}`df_cc100.head()`\n",
    "\n",
    "Based on the dataframe output, the CSV files contain the following columns:\n",
    "- **T** - Time period identifier (e.g., \"2024--P1Y\" represents year 2024 with 1-year period)\n",
    "- **X** - Grid tile X-coordinate in the EPSG:6933 projection system\n",
    "- **Y** - Grid tile Y-coordinate in the EPSG:6933 projection system\n",
    "- **datasets** - Number of Sentinel-2 datasets intersecting this tile\n",
    "- **days** - Number of unique observation days available for this tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": [
     {
      "expression": "df_cc100.describe()",
      "result": {
       "data": {
        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>Y</th>\n      <th>datasets</th>\n      <th>days</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3755.000000</td>\n      <td>3755.000000</td>\n      <td>3755.000000</td>\n      <td>3755.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>228.991212</td>\n      <td>-6.718775</td>\n      <td>277.258589</td>\n      <td>124.297736</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>24.446351</td>\n      <td>13.297015</td>\n      <td>146.041108</td>\n      <td>33.815803</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>182.000000</td>\n      <td>-31.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>208.000000</td>\n      <td>-18.000000</td>\n      <td>152.000000</td>\n      <td>74.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>230.000000</td>\n      <td>-7.000000</td>\n      <td>229.000000</td>\n      <td>145.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>249.000000</td>\n      <td>4.000000</td>\n      <td>383.000000</td>\n      <td>146.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>274.000000</td>\n      <td>16.000000</td>\n      <td>764.000000</td>\n      <td>219.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
        "text/plain": "                 X            Y     datasets         days\ncount  3755.000000  3755.000000  3755.000000  3755.000000\nmean    228.991212    -6.718775   277.258589   124.297736\nstd      24.446351    13.297015   146.041108    33.815803\nmin     182.000000   -31.000000     1.000000     1.000000\n25%     208.000000   -18.000000   152.000000    74.000000\n50%     230.000000    -7.000000   229.000000   145.000000\n75%     249.000000     4.000000   383.000000   146.000000\nmax     274.000000    16.000000   764.000000   219.000000"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{len(df_cc100):,}\"",
      "result": {
       "data": {
        "text/plain": "'3,755'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['datasets'].mean():.2f}\"",
      "result": {
       "data": {
        "text/plain": "'277.26'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['datasets'].median():.0f}\"",
      "result": {
       "data": {
        "text/plain": "'229'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['datasets'].min():.0f}\"",
      "result": {
       "data": {
        "text/plain": "'1'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['datasets'].max():.0f}\"",
      "result": {
       "data": {
        "text/plain": "'764'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['datasets'].std():.2f}\"",
      "result": {
       "data": {
        "text/plain": "'146.04'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['days'].mean():.2f}\"",
      "result": {
       "data": {
        "text/plain": "'124.30'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['days'].median():.0f}\"",
      "result": {
       "data": {
        "text/plain": "'145'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['days'].min():.0f}\"",
      "result": {
       "data": {
        "text/plain": "'1'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['days'].max():.0f}\"",
      "result": {
       "data": {
        "text/plain": "'219'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{df_cc100['days'].std():.2f}\"",
      "result": {
       "data": {
        "text/plain": "'33.82'"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "### 2. The Descriptive Statistics\n",
    "\n",
    "Pandas gives us a statistical snapshot of our data; central tendency (mean, median), spread (std, quartiles), and range (min, max). Perfect for understanding the dataset distribution at a glance.\n",
    "\n",
    "```python\n",
    "df_cc100.describe()\n",
    "```\n",
    "> Here we'll try to extract general information about the dataset `df_cc100`.\n",
    "\n",
    "**Dataframe Statistic Summary:** \\\n",
    "{eval}`df_cc100.describe()`\n",
    "\n",
    ":::{important} Total tiles is **{eval}`f\"{len(df_cc100):,}\"`**\n",
    "\n",
    "We can also observe some more information about the dataset:  \n",
    "\n",
    "1. **Datasets per Tile**\n",
    "\n",
    "    - _Mean_: {eval}`f\"{df_cc100['datasets'].mean():.2f}\"` datasets per tile\n",
    "    - _Median_: {eval}`f\"{df_cc100['datasets'].median():.0f}\"` datasets per tile\n",
    "    - _Range_: {eval}`f\"{df_cc100['datasets'].min():.0f}\"` to {eval}`f\"{df_cc100['datasets'].max():.0f}\"` datasets\n",
    "    - _Standard deviation_: {eval}`f\"{df_cc100['datasets'].std():.2f}\"`\n",
    "\n",
    "2. **Observation Days per Tile**\n",
    "\n",
    "    - _Mean_: {eval}`f\"{df_cc100['days'].mean():.2f}\"` days per tile\n",
    "    - _Median_: {eval}`f\"{df_cc100['days'].median():.0f}\"` days per tile\n",
    "    - _Range_: {eval}`f\"{df_cc100['days'].min():.0f}\"` to {eval}`f\"{df_cc100['days'].max():.0f}\"` days\n",
    "    - _Standard deviation_: {eval}`f\"{df_cc100['days'].std():.2f}\"`\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Create horizontal boxplot\n",
    "bp = ax.boxplot([df_cc100['datasets'], df_cc100['days']], \n",
    "                 vert=False,\n",
    "                 tick_labels=['Datasets', 'Days'],\n",
    "                 patch_artist=True)\n",
    "\n",
    "# Set colors\n",
    "colors = ['#3498db', '#f39c12']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_xlabel('Count', fontsize=12)\n",
    "ax.set_ylabel('Metric', fontsize=12)\n",
    "ax.set_title('Distribution of Datasets and Observation Days per Tile', \n",
    "             fontsize=13, fontweight='bold', pad=15)\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{grid} 2\n",
    ":::{card}\n",
    ":header: **What the Data Shows**\n",
    "\n",
    "Most tiles contain 150-380 datasets spanning 74-146 observation days, with a median of 229 datasets across 145 days. Dataset counts vary significantly more than observation duration, with some tiles receiving multiple satellite passes per day (average 1.6 datasets/day). The spatial distribution is uneven—some areas have substantially more data than others due to satellite orbit patterns.\n",
    ":::\n",
    "\n",
    ":::{card}\n",
    ":header: **Implications for GeoMAD Analysis**\n",
    "\n",
    "Data-rich tiles will produce more robust median and MAD statistics, while tiles with fewer observations (around 74 days) may not capture full seasonal variations for annual analysis. The spatial imbalance means geoMAD composite quality varies across the region. Areas with limited temporal coverage may be more susceptible to outliers or cloud contamination, affecting the reliability of statistical summaries when comparing results between tiles.\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Cloud Cover Filter Analysis\n",
    "\n",
    "We apply the same analytical method to filtered datasets (`tasks_cc60.csv` and `tasks_cc80.csv`) to evaluate the impact of cloud cover thresholds on data availability and quality.\n",
    "\n",
    "\n",
    "- **CC60** applies a more aggressive cloud cover filter (≤60% cloud cover allowed)  \n",
    "- **CC80** applies a more lenient filter (≤80% cloud cover allowed)\n",
    "\n",
    "\n",
    "### 1. The Question\n",
    "\n",
    "**How do we balance output quality with dataset availability?**\n",
    "\n",
    "Larger cloud cover filters reduce the number of cloudy datasets, which should improve output image quality. However, we must maintain a minimum dataset count to provide sufficient information for robust statistical analysis, particularly for **Median Absolute Deviation (MAD)** calculations.\n",
    "\n",
    "\n",
    "### 2. Comparative Analysis\n",
    "\n",
    "We compare two configurations (60% and 80% cloud cover thresholds) across two dimensions:\n",
    "\n",
    "::::{grid} 2\n",
    ":gutter: 3\n",
    "\n",
    ":::{grid-item-card} Dataset Count Analysis\n",
    ":class-header: bg-light\n",
    "\n",
    "- Number of available datasets per tile\n",
    "- Temporal coverage consistency\n",
    "- Spatial distribution patterns\n",
    "- Data retention rates vs. baseline (CC100)\n",
    "\n",
    ":::\n",
    "\n",
    ":::{grid-item-card} Output Quality Assessment\n",
    ":class-header: bg-light\n",
    "\n",
    "- Image clarity and cloud contamination\n",
    "- Statistical robustness (MAD reliability)\n",
    "- Spatial completeness\n",
    "- Temporal representativeness\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "### 3. Expected Trade-offs\n",
    "\n",
    "```{list-table}\n",
    ":header-rows: 1\n",
    ":name: tradeoff-table\n",
    "\n",
    "* - Metric\n",
    "  - CC60 (Aggressive)\n",
    "  - CC80 (Lenient)\n",
    "* - Dataset Count\n",
    "  - Lower\n",
    "  - Higher\n",
    "* - Cloud Contamination\n",
    "  - Minimal\n",
    "  - Moderate\n",
    "* - Statistical Power\n",
    "  - Risk of insufficient data\n",
    "  - Better temporal sampling\n",
    "* - Output Quality\n",
    "  - Cleaner imagery\n",
    "  - Potential artifacts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the other CSV files \n",
    "df_cc80 = pd.read_csv(\"tasks_cc80.csv\")\n",
    "df_cc60 = pd.read_csv(\"tasks_cc60.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": [
     {
      "expression": "df_cc80.describe()",
      "result": {
       "data": {
        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>Y</th>\n      <th>datasets</th>\n      <th>days</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3749.000000</td>\n      <td>3749.000000</td>\n      <td>3749.000000</td>\n      <td>3749.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>229.014137</td>\n      <td>-6.689250</td>\n      <td>162.220592</td>\n      <td>81.488930</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>24.455164</td>\n      <td>13.286465</td>\n      <td>92.374814</td>\n      <td>26.888991</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>182.000000</td>\n      <td>-31.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>208.000000</td>\n      <td>-18.000000</td>\n      <td>92.000000</td>\n      <td>58.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>230.000000</td>\n      <td>-7.000000</td>\n      <td>140.000000</td>\n      <td>86.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>249.000000</td>\n      <td>4.000000</td>\n      <td>223.000000</td>\n      <td>102.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>274.000000</td>\n      <td>16.000000</td>\n      <td>550.000000</td>\n      <td>179.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
        "text/plain": "                 X            Y     datasets         days\ncount  3749.000000  3749.000000  3749.000000  3749.000000\nmean    229.014137    -6.689250   162.220592    81.488930\nstd      24.455164    13.286465    92.374814    26.888991\nmin     182.000000   -31.000000     1.000000     1.000000\n25%     208.000000   -18.000000    92.000000    58.000000\n50%     230.000000    -7.000000   140.000000    86.000000\n75%     249.000000     4.000000   223.000000   102.000000\nmax     274.000000    16.000000   550.000000   179.000000"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "df_cc60.describe()",
      "result": {
       "data": {
        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>Y</th>\n      <th>datasets</th>\n      <th>days</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3749.000000</td>\n      <td>3749.000000</td>\n      <td>3749.000000</td>\n      <td>3749.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>229.014137</td>\n      <td>-6.689250</td>\n      <td>121.142971</td>\n      <td>64.016538</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>24.455164</td>\n      <td>13.286465</td>\n      <td>76.255768</td>\n      <td>25.470309</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>182.000000</td>\n      <td>-31.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>208.000000</td>\n      <td>-18.000000</td>\n      <td>65.000000</td>\n      <td>44.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>230.000000</td>\n      <td>-7.000000</td>\n      <td>103.000000</td>\n      <td>64.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>249.000000</td>\n      <td>4.000000</td>\n      <td>161.000000</td>\n      <td>82.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>274.000000</td>\n      <td>16.000000</td>\n      <td>506.000000</td>\n      <td>172.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
        "text/plain": "                 X            Y     datasets         days\ncount  3749.000000  3749.000000  3749.000000  3749.000000\nmean    229.014137    -6.689250   121.142971    64.016538\nstd      24.455164    13.286465    76.255768    25.470309\nmin     182.000000   -31.000000     1.000000     1.000000\n25%     208.000000   -18.000000    65.000000    44.000000\n50%     230.000000    -7.000000   103.000000    64.000000\n75%     249.000000     4.000000   161.000000    82.000000\nmax     274.000000    16.000000   506.000000   172.000000"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "### 4. Data Availibility and Distribution\n",
    ":::: {grid} 2\n",
    "::: {grid-item}\n",
    "```{code}python\n",
    ":label: descriptive-analysis-cloud-cover-80\n",
    ":caption: descriptive statistics of filtered dataset with cloud-cover 0 to 80%\n",
    "df_cc80.describe()\n",
    "```\n",
    "\n",
    "\n",
    "{eval}`df_cc80.describe()`\n",
    ":::\n",
    "\n",
    "::: {grid-item}\n",
    "```{code}python\n",
    ":label: descriptive-analysis-cloud-cover-60\n",
    ":caption: descriptive statistics of filtered dataset with cloud-cover 0 to 60%\n",
    "df_cc60.describe()\n",
    "```\n",
    "\n",
    "\n",
    "{eval}`df_cc60.describe()`\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw={'wspace': 0.5})\n",
    "\n",
    "colors = ['#3498db', '#f39c12']\n",
    "\n",
    "# Column 1: Datasets comparison\n",
    "bp1 = ax1.boxplot([df_cc80['datasets'], df_cc60['datasets']], \n",
    "                   vert=False,\n",
    "                   tick_labels=['80CC', '60CC'],\n",
    "                   patch_artist=True)\n",
    "\n",
    "for patch, color in zip(bp1['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax1.set_xlabel('Dataset Count per Tile', fontsize=12)\n",
    "ax1.set_ylabel('CC Threshold', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Column 2: Days comparison\n",
    "bp2 = ax2.boxplot([df_cc80['days'], df_cc60['days']], \n",
    "                   vert=False,\n",
    "                   tick_labels=['80CC', '60CC'],\n",
    "                   patch_artist=True)\n",
    "\n",
    "for patch, color in zip(bp2['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax2.set_xlabel('Observation Days Count per Tile', fontsize=12)\n",
    "ax2.set_ylabel('CC Threshold', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cloud Cover Threshold Analysis\n",
    "\n",
    ":::{card}\n",
    ":header: **Overview**\n",
    "- We analyzed three cloud cover thresholds (100%, 80%, 60%) to determine optimal filtering for our satellite imagery dataset.\n",
    "- The analysis reveals that **80% threshold offers the best balance**, though **60% remains acceptable**.\n",
    ":::\n",
    "\n",
    "::::{grid} 1 1 3 3\n",
    ":gutter: 3\n",
    "\n",
    ":::{card} Data Coverage Impact\n",
    "**Location Retention:**\n",
    "- **100%**: 3,755 locations\n",
    "- **80%**: 3,749 locations (99.8%)\n",
    "- **60%**: 3,749 locations (99.8%)\n",
    "\n",
    "**Key Insight:** Loss of only 6 locations indicates nearly all grid points have sufficient usable data, even with stricter cloud filtering.\n",
    ":::\n",
    "\n",
    ":::{card} Dataset Availability\n",
    "**Mean / Median Datasets:**\n",
    "- **100%**: 277 / 229\n",
    "- **80%**: 162 / 140 (-41.5%)\n",
    "- **60%**: 121 / 103 (-56.3%)\n",
    "\n",
    "**Key Insight:** 80% removes ~42% of datasets while maintaining 140 median images per location. 60% removes over half the data but still provides 103 median images - still sufficient for analysis.\n",
    ":::\n",
    "\n",
    ":::{card} Temporal Coverage\n",
    "**Mean / Median Days:**\n",
    "- **100%**: 124 / 145 days\n",
    "- **80%**: 81 / 86 days (-34.7%)\n",
    "- **60%**: 64 / 64 days (-48.5%)\n",
    "\n",
    "**Key Insight:** 80% provides 86 median days - sufficient for seasonal analysis. 60% reduces to 64 days (~2 months coverage) which may create temporal gaps but ensures each observation is high-quality.\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    ":::{card} \n",
    ":header: **Next Steps**\n",
    "To make a final decision, we need to **visually compare outputs** from different thresholds by sampling representative tiles based on dataset and day count distributions.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Testing Strategy\n",
    "\n",
    "### 1. Sampling Method\n",
    "\n",
    "::::{grid} 2\n",
    "\n",
    ":::{grid-item-card} \n",
    ":header: **Method: Stratified Purposive Sampling**\n",
    "\n",
    "This approach combines:\n",
    "- Stratified sampling: Dividing the population into distinct subgroups (strata) based on key characteristics\n",
    "- Purposive sampling: Deliberately selecting samples that represent critical scenarios\n",
    "\n",
    "We stratify based on two metrics:\n",
    "1. Absolute data availability (dataset count and temporal coverage at 80%)\n",
    "2. Reduction magnitude (percentage loss when moving from 80% to 60%)\n",
    "\n",
    "This ensures comprehensive coverage of decision-critical scenarios.\n",
    ":::\n",
    "\n",
    ":::{grid-item-card} \n",
    ":header: **Group Classification**\n",
    "Four distinct strata based on data characteristics:\n",
    "\n",
    "| Group | Characteristics | Sample Size |\n",
    "|-------|----------------|-------------|\n",
    "| **1** | Low Reduction (Baseline)             | 2 tiles |\n",
    "| **2** | Low Absolute + High Reduction        | 3 tiles |\n",
    "| **3** | Medium Absolute + High Reduction   | 3 tiles |\n",
    "| **4** | High Absolute + High Reduction       | 3 tiles |\n",
    "| **Total** | | **11 tiles** |\n",
    "\n",
    "Each group tests a specific scenario critical to the threshold decision.\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "### 2. Group Definitions:\n",
    "\n",
    "::::{grid} 2\n",
    ":::{grid-item-card}\n",
    ":header: **Group 1: Low Reduction - Baseline**\n",
    "\n",
    "**a. Characteristics:**\n",
    "- Dataset count at 80% cloud filter with less than 20% dataset loss\n",
    "- Day observations at 80% cloud filter with less than 20% day observed loss\n",
    "\n",
    "**b. Rationale:**\n",
    "- Naturally clear areas with minimal difference between thresholds.\n",
    "- Provides baseline reference and validates methodology.\n",
    "- These tiles should perform well at both thresholds, serving as a control group to confirm that the processing pipeline functions correctly.\n",
    "\n",
    "**c. Illustration:**\n",
    "```\n",
    "CC80:  Tile X1,Y1 with 230 datasets and 140 days observed\n",
    "       ↓ \n",
    "       ↓ (-23 images, -14 days), reduction: ~10% datasets, ~10% days\n",
    "       ↓\n",
    "CC60:  Tile X1,Y1 becomes 207 images, 126 days\n",
    "                   \n",
    "Minimal impact for both thresholds\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{grid-item-card}\n",
    ":header: **Group 2: Low Absolute + High Reduction**\n",
    "\n",
    "**a. Characteristics:**\n",
    "- Dataset count at 80% cloud filter in the bottom 33rd percentile\n",
    "- Day observations at 80% cloud filter in the bottom 33rd percentile\n",
    "- Tiles experiencing the largest reduction in dataset count\n",
    "\n",
    "**b. Rationale:**\n",
    "- Represents the worst-case scenario: areas with limited data availability that experience substantial data loss when transitioning to 60% threshold.\n",
    "- Every image is critical in these sparse areas, yet the threshold change eliminates 40-50% of available data.\n",
    "- If the 60% threshold remains viable under these conditions, it demonstrates robustness across the full data spectrum.\n",
    "\n",
    "**c. Illustration:**\n",
    "```\n",
    "CC80:  Tile X2,Y2 with 60 datasets and 35 days observed\n",
    "       ↓ \n",
    "       ↓ (-30 images, -17 days), reduction: ~50% datasets, ~49% days\n",
    "       ↓\n",
    "CC60:  Tile X2,Y2 becomes 30 images, 18 days\n",
    "                   \n",
    "Severe data loss in sparse area with extreme temporal gaps\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{grid-item-card}\n",
    ":header: **Group 3: Medium Absolute + High Reduction**\n",
    "\n",
    "**a. Characteristics:**\n",
    "- Dataset count at 80% cloud filter within ±20% of the median (170-220 images)\n",
    "- Day observations at 80% cloud filter between 33rd and 67th percentile\n",
    "- Reduction of 25-40% in both dataset count and day observations\n",
    "\n",
    "**b. Rationale:**\n",
    "- Represents the typical scenario: tiles with moderate data availability and moderate reduction.\n",
    "- These tiles exhibit characteristics representative of the broader dataset, demonstrating the standard trade-off between data quantity and quality.\n",
    "- Performance in this group indicates expected outcomes for the majority of the study area.\n",
    "\n",
    "**c. Illustration:**\n",
    "```\n",
    "CC80:  Tile X3,Y3 with 195 datasets and 115 days observed\n",
    "       ↓ \n",
    "       ↓ (-60 images, -35 days), reduction: ~31% datasets, ~30% days\n",
    "       ↓\n",
    "CC60:  Tile X3,Y3 becomes 135 images, 80 days\n",
    "                   \n",
    "Typical trade-off scenario with acceptable coverage and improved quality\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{grid-item-card}\n",
    ":header: **Group 4: High Absolute + High Reduction**\n",
    "\n",
    "**a. Characteristics:**\n",
    "- Dataset count at 80% cloud filter in the top 33rd percentile (220-300+ images)\n",
    "- Day observations at 80% cloud filter in the top 33rd percentile\n",
    "- Reduction of 40-50%+ in both dataset count and day observations\n",
    "\n",
    "**b. Rationale:**\n",
    "- Represents data-rich areas that lose substantial absolute numbers of images (50-100+) but retain sufficient data for analysis.\n",
    "- These tiles contain many images with 60-80% cloud cover, presenting the maximum potential for quality improvement.\n",
    "- Tests whether quality enhancement justifies significant data reduction when data abundance permits such loss.\n",
    "\n",
    "**c. Illustration:**\n",
    "```\n",
    "CC80:  Tile X4,Y4 with 280 datasets and 165 days observed\n",
    "       ↓ \n",
    "       ↓ (-125 images, -70 days), reduction: ~45% datasets, ~42% days\n",
    "       ↓\n",
    "CC60:  Tile X4,Y4 becomes 155 images, 95 days\n",
    "                   \n",
    "Large absolute reduction with abundant data remaining\n",
    "```\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from matplotlib.patches import Rectangle, Patch\n",
    "import contextily as ctx\n",
    "\n",
    "# Load your GeoJSON file\n",
    "gdf = gpd.read_file('tasks_cc100-2024--P1Y.geojson')\n",
    "\n",
    "# Define thresholds based on 100% cloud cover statistics (using quartiles from your data)\n",
    "total_low = 152    # 25th percentile\n",
    "total_high = 383   # 75th percentile\n",
    "days_low = 74      # 25th percentile\n",
    "days_high = 146    # 75th percentile\n",
    "\n",
    "# Function to categorize total\n",
    "def categorize_total(value):\n",
    "    if value < total_low:\n",
    "        return 'Low'\n",
    "    elif value <= total_high:\n",
    "        return 'Med'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "# Function to categorize days\n",
    "def categorize_days(value):\n",
    "    if value < days_low:\n",
    "        return 'Low'\n",
    "    elif value <= days_high:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "# Apply categorization\n",
    "gdf['total_cat'] = gdf['total'].apply(categorize_total)\n",
    "gdf['days_cat'] = gdf['days'].apply(categorize_days)\n",
    "gdf['category'] = gdf['days_cat'] + '_' + gdf['total_cat']\n",
    "\n",
    "# Define color mapping for 3x3 matrix\n",
    "color_map = {\n",
    "    'Low_Low': '#fee5d9',      # Light red\n",
    "    'Low_Med': '#fcae91',      # Medium red\n",
    "    'Low_High': '#fb6a4a',     # Dark red\n",
    "    'Medium_Low': '#deebf7',   # Light blue\n",
    "    'Medium_Med': '#9ecae1',   # Medium blue\n",
    "    'Medium_High': '#3182bd',  # Dark blue\n",
    "    'High_Low': '#e5f5e0',     # Light green\n",
    "    'High_Med': '#a1d99b',     # Medium green\n",
    "    'High_High': '#31a354',    # Dark green\n",
    "}\n",
    "\n",
    "gdf['color'] = gdf['category'].map(color_map)\n",
    "\n",
    "# Convert to Web Mercator for OpenStreetMap overlay\n",
    "gdf_web = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Function to plot with selected tiles highlighted\n",
    "def plot_with_selected_tiles(gdf, gdf_web, selected_tiles=None, highlight_color='red', highlight_linewidth=3):\n",
    "    \"\"\"\n",
    "    Plot the grid with selected tiles highlighted.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Original geodataframe\n",
    "    gdf_web : GeoDataFrame\n",
    "        Geodataframe in Web Mercator projection\n",
    "    selected_tiles : list of tuples\n",
    "        List of (ix, iy) tuples to highlight. Example: [(182, 0), (182, 1), (183, 0)]\n",
    "    highlight_color : str\n",
    "        Color for the highlight border (default: 'red')\n",
    "    highlight_linewidth : float\n",
    "        Width of the highlight border (default: 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(20, 15))\n",
    "    \n",
    "    # Plot the grid cells with colors (increased alpha)\n",
    "    gdf_web.plot(ax=ax, color=gdf_web['color'], edgecolor='black', linewidth=0.5, alpha=0.4)\n",
    "    \n",
    "    # Highlight selected tiles if provided\n",
    "    if selected_tiles is not None and len(selected_tiles) > 0:\n",
    "        # Create a mask for selected tiles\n",
    "        mask = gdf.apply(lambda row: (row['ix'], row['iy']) in selected_tiles, axis=1)\n",
    "        selected_gdf = gdf_web[mask]\n",
    "        \n",
    "        # Plot selected tiles with thicker border\n",
    "        selected_gdf.plot(ax=ax, facecolor='none', edgecolor=highlight_color, \n",
    "                         linewidth=highlight_linewidth, alpha=1.0)\n",
    "        \n",
    "        print(f\"\\nHighlighted {len(selected_gdf)} tiles:\")\n",
    "        print(\"=\"*60)\n",
    "        for _, row in gdf[mask].iterrows():\n",
    "            print(f\"  Tile: {row['title']} (ix={row['ix']}, iy={row['iy']}) - \"\n",
    "                  f\"Category: {row['category']} - Total: {row['total']}, Days: {row['days']}\")\n",
    "    \n",
    "    # Add OpenStreetMap basemap\n",
    "    ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, alpha=0.5)\n",
    "    \n",
    "    # Add title and labels\n",
    "    title = 'Grid Cells Categorized by Total and Days\\n(100% Cloud Cover Threshold)'\n",
    "    if selected_tiles is not None and len(selected_tiles) > 0:\n",
    "        title += f'\\n{len(selected_tiles)} tiles highlighted in {highlight_color}'\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Get unique ix and iy values sorted\n",
    "    ix_values = sorted(gdf['ix'].unique())\n",
    "    iy_values = sorted(gdf['iy'].unique())\n",
    "    \n",
    "    # Calculate approximate positions for tick labels\n",
    "    ix_positions = {}\n",
    "    iy_positions = {}\n",
    "    \n",
    "    for ix in ix_values:\n",
    "        tiles = gdf_web[gdf['ix'] == ix]\n",
    "        if len(tiles) > 0:\n",
    "            ix_positions[ix] = tiles.geometry.centroid.x.mean()\n",
    "    \n",
    "    for iy in iy_values:\n",
    "        tiles = gdf_web[gdf['iy'] == iy]\n",
    "        if len(tiles) > 0:\n",
    "            iy_positions[iy] = tiles.geometry.centroid.y.mean()\n",
    "    \n",
    "    # Set custom ticks - sample every Nth value to avoid overcrowding\n",
    "    n_ticks = 20\n",
    "    ix_step = max(1, len(ix_values) // n_ticks)\n",
    "    iy_step = max(1, len(iy_values) // n_ticks)\n",
    "    \n",
    "    selected_ix = ix_values[::ix_step]\n",
    "    selected_iy = iy_values[::iy_step]\n",
    "    \n",
    "    # Set x-axis ticks\n",
    "    ax.set_xticks([ix_positions[ix] for ix in selected_ix if ix in ix_positions])\n",
    "    ax.set_xticklabels([f'ix={ix}' for ix in selected_ix if ix in ix_positions], rotation=45, ha='right')\n",
    "    ax.set_xlabel('Tile Index X (ix)', fontsize=12)\n",
    "    \n",
    "    # Set y-axis ticks\n",
    "    ax.set_yticks([iy_positions[iy] for iy in selected_iy if iy in iy_positions])\n",
    "    ax.set_yticklabels([f'iy={iy}' for iy in selected_iy if iy in iy_positions])\n",
    "    ax.set_ylabel('Tile Index Y (iy)', fontsize=12)\n",
    "    \n",
    "    # Create legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#fee5d9', edgecolor='black', label='Low Days / Low Total'),\n",
    "        Patch(facecolor='#fcae91', edgecolor='black', label='Low Days / Med Total'),\n",
    "        Patch(facecolor='#fb6a4a', edgecolor='black', label='Low Days / High Total'),\n",
    "        Patch(facecolor='#deebf7', edgecolor='black', label='Medium Days / Low Total'),\n",
    "        Patch(facecolor='#9ecae1', edgecolor='black', label='Medium Days / Med Total'),\n",
    "        Patch(facecolor='#3182bd', edgecolor='black', label='Medium Days / High Total'),\n",
    "        Patch(facecolor='#e5f5e0', edgecolor='black', label='High Days / Low Total'),\n",
    "        Patch(facecolor='#a1d99b', edgecolor='black', label='High Days / Med Total'),\n",
    "        Patch(facecolor='#31a354', edgecolor='black', label='High Days / High Total'),\n",
    "    ]\n",
    "    \n",
    "    # Add highlight indicator to legend if tiles are selected\n",
    "    if selected_tiles is not None and len(selected_tiles) > 0:\n",
    "        legend_elements.append(Patch(facecolor='none', edgecolor=highlight_color, \n",
    "                                     linewidth=highlight_linewidth, label='Selected Tiles'))\n",
    "    \n",
    "    ax.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# Example usage 1: Plot without selection\n",
    "print(\"Plotting all tiles without selection...\")\n",
    "plot_with_selected_tiles(gdf, gdf_web)\n",
    "\n",
    "# Example usage 2: Plot with selected tiles\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Plotting with selected tiles highlighted...\")\n",
    "selected_tiles = [\n",
    "    (182, -1),\n",
    "    (182, 0),\n",
    "    (182, 1),\n",
    "    (183, 0),\n",
    "    (184, 0)\n",
    "]\n",
    "plot_with_selected_tiles(gdf, gdf_web, selected_tiles=selected_tiles, \n",
    "                        highlight_color='red', highlight_linewidth=4)\n",
    "\n",
    "# Example usage 3: Different highlight style\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Plotting with different highlight style...\")\n",
    "other_selected = [\n",
    "    (185, 2),\n",
    "    (186, 2),\n",
    "    (187, 3)\n",
    "]\n",
    "plot_with_selected_tiles(gdf, gdf_web, selected_tiles=other_selected, \n",
    "                        highlight_color='yellow', highlight_linewidth=5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
